{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression and Regularization\n",
    "Loosely following Chapter 10 of Python Machine Learning 3rd Edition, Raschka\n",
    "\n",
    ">Disclaimer: Regression is a huge field. It is impossible to cover it all in one class (or even two).\n",
    "\n",
    "<img src='files/diagrams/ols.png' style='width: 500px'>\n",
    "\n",
    "[Image Source](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html)\n",
    "\n",
    ">For a sense of the depth and potential complexity of regression models, see [Mostly Harmless Econometrics](https://www.mostlyharmlesseconometrics.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda:\n",
    "- What is least squares regression?  \n",
    "- Closed-form solution.  \n",
    "- Evaluation metrics.  \n",
    "- Reading regression output.  \n",
    "- p-value hacking.  \n",
    "- Checking residuals.  \n",
    "- Predicting new values.  \n",
    "- Robust regression methods.  \n",
    "- Multiple regression. \n",
    "- Strengths and weaknesses.  \n",
    "- Pipelines.  \n",
    "- Ridge.\n",
    "- Gradient descent.  \n",
    "- Lasso.\n",
    "\n",
    "\n",
    "\n",
    "### Resources:\n",
    "<br>[Ben Lambert's Derivation of OLS](https://www.youtube.com/watch?v=fb1CNQT-3Pg)\n",
    "<br>[Andrew Ng Regression Lecture](https://www.youtube.com/watch?v=4b4MUYve_U8)\n",
    "<br>[Cornell Regression Lecture](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote08.html)\n",
    "<br>[Deriving Ridge Regression](https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution)\n",
    "<br>[Common Pitfalls in Interpreting Coefficients](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html)\n",
    "<br>[HBR Article - Beware Spurious Correlations](https://hbr.org/2015/06/beware-spurious-correlations)\n",
    "<br>[Saving your models with pickle](https://scikit-learn.org/stable/modules/model_persistence.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares (OLS) Linear Regression\n",
    "\n",
    ">[All models are wrong, but some are useful.](https://en.wikipedia.org/wiki/All_models_are_wrong)\n",
    "<br><br>George Box\n",
    "\n",
    "Linear regression is one of the most popular, widely used, and foundational concepts in statistics, econometrics, and machine learning.  \n",
    "\n",
    "Boils down having a numeric target value ($y$) we want to either predict or understand the variance drivers. We use data ($X$) we think impacts our target to understand the underlying **linear** relationship.\n",
    "\n",
    "### Big Assumption: \n",
    "- The regression function $E(Y|X)$ is **linear** in the inputs $X_{1},\\dots,X_{p}$\n",
    "- Transformations can be applied to satisfy that requirement.\n",
    "\n",
    "Typically will see it expressed as $y = \\beta X$, or formally:  \n",
    "\n",
    "$$\n",
    "f(X)=\\beta_{0}+\\sum{X{j}\\beta_{j}}\n",
    "$$\n",
    "\n",
    "- Linear model assumes the function is linear or reasonably linear.\n",
    "- The true $\\beta_{j}$'s are unknown parameters (coefficients/weights). \n",
    "- The features must be able to be represented within a non-null numeric matrix.\n",
    "\n",
    "\n",
    "### Goal - Minimize the mean-squared error. Why?\n",
    "Residuals will be positive and negative, need to penalize negative and positive equally.\n",
    "\n",
    "Sum of errors: $\\epsilon_{1} + \\epsilon_{2} + \\dots + \\epsilon_{n}$  \n",
    "RSS: $\\epsilon_{1}^2 + \\epsilon_{2}^2 + \\dots + \\epsilon_{n}^2$  \n",
    "MSE: $\\frac{1}{N}\\sum{\\epsilon_{i}^2}$\n",
    "\n",
    "\n",
    "Most statistical programs solve for the $\\beta$ values using plain old linear alegbra, in what is called the closed-form:  \n",
    "\n",
    "$\\hat\\beta = (X^TX)^{-1}X^{T}y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed Form Derivation\n",
    "\n",
    "$$RSS(\\beta)=\\sum{(y_{i}-f(x_{i}))^2}$$\n",
    "\n",
    "$$=\\sum(y_{i}-\\beta_{0}-\\sum{x_{ij}\\beta{j}})^2$$\n",
    "\n",
    "$(x_i,y_i)$ should be independent from other $(x_i,y_i)$'s  \n",
    "- Time series models violate this without special treatment\n",
    "\n",
    "We are seeking a $f(X)$ that minimizes the sum of squared residuals from $Y$:\n",
    "\n",
    "<img src='files/diagrams/esl-3-1.png' style='width: 500px'>\n",
    "\n",
    "\n",
    "[Image source: Elements of Statistical Learning, Figure 3.1](https://www.statlearning.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "RSS(\\beta)=(y-X\\beta)^T(y-X\\beta)\n",
    "$$\n",
    "\n",
    "#### Differentiating:\n",
    "$$\n",
    "\\frac{dRSS}{d\\beta}=-2X^T(y-X\\beta)\n",
    "$$\n",
    "\n",
    "#### And again:\n",
    "$$\n",
    "\\frac{d^2RSS}{d\\beta d \\beta^T}=2X^TX\n",
    "$$\n",
    "\n",
    "#### Setting the first derivative to zero:\n",
    "$$ \n",
    "X^T(y-X\\beta)=0\n",
    "$$\n",
    "\n",
    "#### And we get:\n",
    "$$\n",
    "\\hat{\\beta}=(X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "#### And our predicted values are:\n",
    "$$\n",
    "\\hat{y}=X\\hat{\\beta}\n",
    "$$\n",
    "\n",
    "#### And relates to $y$ by:\n",
    "$$\n",
    "y = \\hat{y} + \\epsilon =X\\hat{\\beta}+\\epsilon \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Unique solution means we can derive with pure linear algebra, i.e., no need to use convergence algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slope and Intercept\n",
    "### Remember in its simple form: $$y=mx+b$$\n",
    "\n",
    "<img src='files/diagrams/slope.png' style=\"height: 200px;width: 200px;\">\n",
    "\n",
    "[Image source](https://en.wikipedia.org/wiki/Linear_regression#/media/File:Linear_least_squares_example2.png)\n",
    "\n",
    "> Since we need an estimate for the intercept, we'll need to manually add the constant. Not all implementations do this automatically, e.g., statsmodels.\n",
    "\n",
    "- Intercept: where the line go through the y-axis.  \n",
    "- Slope: for a 1-unit change in x, y will increase by $\\beta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Credit Data\n",
    "[Data from Elements of Statistical Learning](https://www.statlearning.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "credit = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/regression/islr-credit.csv')\n",
    "credit = credit.iloc[:, 1:]\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Function so $Rating=f(Limit)$\n",
    "We'll need to convert the pandas objects to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.plot.scatter(x='Limit', y='Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression using closed-form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(credit['Limit']).reshape(-1,1)\n",
    "y = np.array(credit['Rating']).reshape(-1,1)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since we are going to implement a version, we'll need to manually add the constant for the intercept. Why?\n",
    "\n",
    "$y=\\beta_{0}(1)+\\beta_{i}x_{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "'''\n",
    "- Manually adding the constant\n",
    "- Sometimes this is done via the API (check the docs)\n",
    "''' \n",
    "const = np.ones(shape=y.shape)\n",
    "mat = np.concatenate( (const, X), axis=1)\n",
    "\n",
    "# first 5 examples\n",
    "mat[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betas\n",
    "We have a feature matrix that has 2 columns, so we'll get estimate for the constant ($\\beta_{0}$) and the credit limit ($\\beta_{1}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the coefficient estimates\n",
    "Recall $\\hat\\beta = (X^TX)^{-1}X^{T}y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = inv(mat.transpose().dot(mat)).dot(mat.transpose()).dot(y)\n",
    "b0, b1 = betas\n",
    "\n",
    "print(f'Beta 0: {np.round(b0[0],3)}')\n",
    "print(f'Beta 1: {np.round(b1[0],3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict $\\hat{y}$ and plot the fit\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{y}=\\hat{\\beta}X=\n",
    "\\hat{\\beta_{0}}\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "\\dots \\\\\n",
    "1\n",
    "\\end{pmatrix}+\n",
    "\\hat{\\beta_{1}}\n",
    "\\begin{pmatrix}\n",
    "3606 \\\\\n",
    "\\dots \\\\\n",
    "5524\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = mat.dot(betas)\n",
    "\n",
    "plt.plot(X, y, 'bo')\n",
    "plt.plot(X, yhat, 'r')\n",
    "plt.xlabel('Credit Limit')\n",
    "plt.ylabel('Credit Rating')\n",
    "plt.title('$Rating=f(Limit)$', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantifying fit with metrics\n",
    "Common metrics:\n",
    "\n",
    "### $R^2$ [Wikipedia](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
    "$$1 - \\frac{\\sum (\\hat {y}-y)^{2}}{\\sum ({\\bar y}-y)^{2}}$$\n",
    "\n",
    "### Mean squared error (MSE) [Wikipedia](https://en.wikipedia.org/wiki/Mean_squared_error)\n",
    "$$\\frac{\\sum (\\hat {y}-y)^{2}}{n}$$\n",
    "\n",
    "### Mean Absolute Error (MAE) [Wikipedia](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
    "$$1/n\\sum |\\hat {y}-y|$$\n",
    "\n",
    "### Root mean squared error (RMSE) [Wikipedia](https://en.wikipedia.org/wiki/Root_mean_square)\n",
    "$$\\sqrt \\frac{\\sum (\\hat {y}-y)^{2}}{n}$$\n",
    "\n",
    "#### Notes:\n",
    "- $r^2$ expresses the percent of variance explained (bound between 0% and 100%).  \n",
    "- RMSE expresses the variance in unit terms.  \n",
    "- MSE/MAE are heavily influenced by outliers.  \n",
    "- Usually RMSE is chosen for optimizing since it's an unbiased estimator.\n",
    "- If there are a lot of outliers, MAE may be a better choice.\n",
    "\n",
    "### Further reading:\n",
    "[$r^2$ vs. RMSE](https://www.statology.org/rmse-vs-r-squared/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrepretability\n",
    "A nice property of linear regression is the relatively simple intrepretations that can be drawn. The implementation in statsmodels includes all the output needed to evaluate and interpret model output.\n",
    "\n",
    "[statsmodels OLS regression](https://www.statsmodels.org/stable/regression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# betas we calculated:\n",
    "betas.reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statsmodels output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as smf\n",
    "\n",
    "simpleModel = smf.OLS(y, mat).fit()\n",
    "print(simpleModel.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Discussion:\n",
    "- Overall $r^2$ of 99% - very strong linear relationship as we saw in the initial plot we created.  \n",
    "- You can ignore the t-statistic on the constant - it isn't meaningful.  \n",
    "- The t-statistic for the credit limit (x1) is very high (252), with a [p-value](https://en.wikipedia.org/wiki/P-value) of essentially 0 - recall a p-value is the probably that the effect we are seeing is by random chance.  \n",
    "- We can conclude there is a very strong linear relationship.\n",
    "\n",
    "### Further reading:\n",
    "[Rethinking p-values](https://www.vox.com/2016/3/15/11225162/p-value-simple-definition-hacking)  \n",
    "[Econometrics](https://en.wikipedia.org/wiki/Econometrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Note on p-Hacking\n",
    "\n",
    "<img src='files/diagrams/bad-coef.png' style=\"width: 500px;\">\n",
    "\n",
    ">Taken from \"Having Too Little or Too Much Time Is Linked to Lower Subjective Well-Being\", Sharif et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn's LinearRegression\n",
    "[scikit-learn's LinearRegression](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)\n",
    "\n",
    "This will probably be the implementation you'd want to use for comparing various regression models for prediction problems since the API will be similar.\n",
    "\n",
    "scikit-learn is geared more towards prediction and won't have some of the friendly output that statsmodels has - if you are going for an intrepretation, you may be better off using statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# we added the intercept manually, so turn that option off\n",
    "skOLS = LinearRegression(fit_intercept=False).fit(mat,y)\n",
    "skOLS.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And they all provide the same coefficient/model estimates - assuming the same set-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(betas.reshape(1,-1)[0])\n",
    "print(simpleModel.params)\n",
    "print(skOLS.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weakness - Outlier Sensitivity\n",
    "There aren't any outliers in the data, I'm going to introduce one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matCopy = mat.copy()\n",
    "matCopy[4, 1] = matCopy[4, 1] + 150000\n",
    "\n",
    "outModel = smf.OLS(y, matCopy).fit()\n",
    "print(outModel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_out = outModel.predict(matCopy)\n",
    "\n",
    "plt.plot(matCopy[:,1], y, 'bo')\n",
    "plt.plot(matCopy[:,1], yhat_out, 'r')\n",
    "plt.xlabel('Credit Limit')\n",
    "plt.ylabel('Actual / Expected Credit Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the sensitivity?\n",
    "Recall we are minimizing the sum of squared residuals. That really big outlier is going to a lot of influence.  \n",
    "\n",
    "## How to combat?\n",
    "- [RANdom SAmple Consensus (RANSAC)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html)  \n",
    "- Replace or remove the outliers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANSAC\n",
    "- Select random samples. \n",
    "- Tests non-sample points and creates a inlier list (opposite of outlier).  \n",
    "- Refits models with all inliers.  \n",
    "- Evaluates error.  \n",
    "- Stops or repeats until iterations/threshold met.\n",
    "- **Not guaranteed to get same answer each time* - why?**\n",
    "\n",
    "> [More details](https://scikit-learn.org/stable/modules/linear_model.html#ransac-regression)\n",
    "\n",
    "<img src='files/diagrams/ransac.png'>\n",
    "\n",
    "[Image Source](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ransac.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "ransac = RANSACRegressor().fit(matCopy, y)\n",
    "yhat_ransac = ransac.predict(matCopy)\n",
    "\n",
    "plt.plot(matCopy[:,1], y, 'bo')\n",
    "plt.plot(matCopy[:,1], yhat_ransac, 'r')\n",
    "plt.xlabel('Credit Limit')\n",
    "plt.ylabel('Actual / Expected Credit Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strength: Robust to Overfitting\n",
    ">Simple is better than complex.\n",
    "\n",
    "### No Overfitting\n",
    "<img src='files/diagrams/reg.png'  style=\"width: 400px;\"> \n",
    "\n",
    "### Severe Overfitting\n",
    "<img src='files/diagrams/poly.png'  style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression\n",
    "Instead of an $mx1$ input matrix, we'll have $mxn$.\n",
    "\n",
    "### $y = w_{0} + w_{1}x_{1} + \\dots + w_{m}x_{m} = \\sum{w_{i}x_{i}}=w^{T}x$\n",
    "\n",
    "Coefficients still reference the effect on $y$ of a 1-unit change to a $x$ - all else held constant.\n",
    "\n",
    "Example with the California housing dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "    x0, x1 = X.shape\n",
    "\n",
    "print(f'Rows: {x0:,}\\nFeatures: {x1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_df = pd.concat([X,y], axis=1)\n",
    "california_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to add a constant/intercept term manually as statsmodels doesn't handle this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_df['const'] = 1\n",
    "\n",
    "featureNames = [x for x in california_df.columns if x != 'MedHouseVal']\n",
    "featureNames = ['const'] + list(featureNames)\n",
    "\n",
    "print(featureNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Double and triple check that you properly separated your target variable from your features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as smf\n",
    "statsModelsCoefs = smf.OLS(california_df['MedHouseVal'], california_df[featureNames]).fit()\n",
    "\n",
    "print(statsModelsCoefs.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = statsModelsCoefs.predict(california_df[featureNames])\n",
    "resid = y - yhat\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(y, yhat, 'ro')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.hist(resid)\n",
    "plt.title('Residual Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns in residuals are not ideal. You'll want these to look like normally distributed white noise (ideally). We might be able to make those residuals behave with feature transformations and other techniques we'll talk about later.\n",
    "\n",
    ">In certain cases, it may help to log-transform your target variable, which will compress some of the variance.\n",
    "\n",
    "[Log-linear models](https://en.wikipedia.org/wiki/Log-linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as smf\n",
    "logMV = np.log(california_df['MedHouseVal'])\n",
    "\n",
    "statsModelsCoefs = smf.OLS(logMV, california_df[featureNames]).fit()\n",
    "print(statsModelsCoefs.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logyhat = statsModelsCoefs.predict(california_df[featureNames])\n",
    "logresid = logMV - logyhat\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(np.exp(logMV), np.exp(logyhat), 'ro', alpha=0.1)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We'll go over models later that can catch the tail of this data better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we had categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role of [Dummy Variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics))\n",
    "A way to incorporate categorical data into modeling, since models require numerical matrices. Essentially acts to change the intercept.\n",
    "\n",
    "### Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.DataFrame([[2,4,1],\n",
    "                     [3,6,1],\n",
    "                     [4,8,1],\n",
    "                     [6,12,1],\n",
    "                     [7,14,1],\n",
    "                     [2,6,0],\n",
    "                     [4,10,0],\n",
    "                     [6,14,0],\n",
    "                     [7,16,0],\n",
    "                     [3,8,0]], columns=['hgt', 'wgt', 'label'])\n",
    "\n",
    "\n",
    "dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistent differences between these lines. Track parallel to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1 = dummy.query('label==1')\n",
    "class2 = dummy.query('label==0')\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(class1['hgt'], class1['wgt'], 'bo')\n",
    "plt.plot(class2['hgt'], class2['wgt'], 'ro')\n",
    "plt.legend(['Label = 1', 'Label = 0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the means for the data by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.groupby('label')['wgt'].mean().diff()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare running a model with and without a dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "Xa = np.array(dummy['hgt']).reshape(-1,1)\n",
    "Xb = np.array(dummy[['hgt','label']])\n",
    "\n",
    "y = np.array(dummy['wgt']).reshape(-1,1)\n",
    "\n",
    "bothOLS = LinearRegression().fit(Xa,y)\n",
    "yhat_both = bothOLS.predict(Xa)\n",
    "\n",
    "sepOLS = LinearRegression().fit(Xb, y)\n",
    "yhat_sep = sepOLS.predict(Xb)\n",
    "\n",
    "print('No Dummy:\\n')\n",
    "print(f' Intercept: {np.round(bothOLS.intercept_[0],2)}')\n",
    "print(f' Slope: {np.round(bothOLS.coef_[0][0],2)}\\n')\n",
    "\n",
    "print('w/ Dummy:\\n')\n",
    "print(f' Intercept: {np.round(sepOLS.intercept_[0], 2)}')\n",
    "print(f' Slope: {np.round(sepOLS.coef_[0][0],2)}')\n",
    "print(f' Dummy: {np.round(sepOLS.coef_[0][1], 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dummy captures the mean difference! Otherwise the slope is identical.\n",
    "\n",
    "If the dummy was present:\n",
    "$$\n",
    "y=1.0 + (2)(x_1) + (2)(1) \n",
    "$$\n",
    "\n",
    "If the dummy was not present:\n",
    "$$\n",
    "y=2.0 + (2)(x_1) + (2)(0)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Categorical Variables into a Model w/ Pipelines\n",
    "[Example from \"Hands-on Machine Learning with Scikit-Learn, Keras & Tensorflow\"](https://github.com/ageron/handson-ml2)\n",
    "\n",
    "This is an expanded, rawer form of the california housing data that is available in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "housing = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/regression/housing.csv')\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal - Predict Median House Value\n",
    "\n",
    "#### Things we need to consider:\n",
    "- Ocean Proximity is categorical. \n",
    "- Missing values in Total Bedrooms.  \n",
    "- Data of significantly different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.median_house_value.hist()\n",
    "plt.title('Distribution of Median Home Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.hist(bins=50, figsize=(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting items of note:\n",
    "- Wide variances in scales. \n",
    "- Median house value truncated at $500,000  \n",
    "- Outliers are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.groupby('ocean_proximity')['median_house_value'].median().plot.barh()\n",
    "plt.xlabel('Median of Median Home Value')\n",
    "plt.ylabel('')\n",
    "plt.title('Price differences by Ocean Proximity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inland homes have significantly lower prices than homes closer to the water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4, s=housing['population']/100,\n",
    "            label='population', figsize=(10,7), c='median_house_value', cmap=plt.get_cmap('jet'),\n",
    "            colorbar=True)\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher values are largely clustered around the coast.\n",
    "\n",
    "[Example from \"Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow](https://github.com/ageron/handson-ml2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values\n",
    "\n",
    "#### Options:\n",
    "- Drop rows (can be bad, what if incoming data has missing values?). \n",
    "- Drop columns (could be bad, what if there value in that feature?).  \n",
    "- Fill in the missing values (ding ding).  \n",
    "- If categorical, might want to add a dummy to indicate it was missing (ding ding).  \n",
    "- Best strategy will be situationally dependent. This can be treated as a hyperparameter - no perfect answer.\n",
    "\n",
    "#### Strategies:\n",
    "- Simple inputers with median, median, mode, random values.  \n",
    "- Estimate the missing value with another machine learning model (increasing overal complexity).  \n",
    "\n",
    ">Not all strategies will work for all data types, so you may need to split it up, e.g., one method for the numerical variables and another for the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "example_imputer = SimpleImputer(strategy='median')\n",
    "example_imputer.fit_transform(np.array(housing.total_bedrooms).reshape(-1,1))\n",
    "\n",
    "example_imputer = pd.Series(example_imputer)\n",
    "example_imputer.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you wanted to add an indictor for the missing value\n",
    "> Probably more useful for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "example_imputer = SimpleImputer(strategy='median', add_indicator=True)\n",
    "example_imputer.fit_transform(np.array(housing.total_bedrooms).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on One-Hot Encoding\n",
    "\n",
    "From \"Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow\":\n",
    "> If a categorical attribute has a large number of possible categories, then one-hot encoding will result in a large number of input features. This may slow down training and degrade performance.\n",
    "\n",
    "#### Possible alternatives in that situation:\n",
    "- Recode to a numerical feature, e.g., distance to ocean.  \n",
    "- Only use the most frequent $N$ categories.  \n",
    "- Convert to embeddings.  \n",
    "\n",
    "#### A risk for us:\n",
    "May not have any islands in the training data, what would happen if we encountered that in our test/evaluation data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.ocean_proximity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Islands are really rare\n",
    "\n",
    "- Adding a dummy for this won't do much - it'll basically be zero.  \n",
    "- Replace to nearest category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "example_ohe = OneHotEncoder()\n",
    "example_ohe = example_ohe.fit_transform(np.array(housing['ocean_proximity']).reshape(-1,1))\n",
    "example_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sparse matrix returns lists of coordinates in the matrix with a non-zero. It's a more efficient structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_ohe[:5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Can be converted back to a dense format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ohe.toarray()[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should we use this for modeling?\n",
    "\n",
    ">In statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. In this situation, the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multivariate regression model with collinear predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others.\n",
    "<br><br>[Wikipedia](https://en.wikipedia.org/wiki/Multicollinearity)\n",
    "\n",
    "### Could argue that this could be represented as an ordinal variable (island > near ocean > near bay > ...)\n",
    "See [OrdinalEncoder for as example](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)\n",
    "\n",
    "You could try this using both methods to see if one is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Rant on LabelEncoder\n",
    "Should not be used on your feature set! This is commonly done on Kaggle (incorrectly!). Do not use this for your feature processing!\n",
    "\n",
    "[LabelEncoder Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html?highlight=labelencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Numerical Variables\n",
    "Don't skip this - most models don't perform well when variables are on different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.select_dtypes(['float','integer']).describe().round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two main methods:\n",
    "> Only fit these to the training data, no leaking information from the test set!\n",
    "\n",
    "\n",
    "### Min-max scaling\n",
    "- Simple  \n",
    "- Bound between 0 and 1 - a lot of algorithms like that, especially neural networks\n",
    "- scikit-learn gives you some additional flexibility in terms of the range  \n",
    "- Very susceptible to outliers\n",
    "\n",
    "$$\n",
    "x_{scaled} = \\frac{x - x_{min}}{x_{max}-x_{min}}\n",
    "$$\n",
    "\n",
    "### Standardization\n",
    "- Little more involved. \n",
    "- More robust to outliers. \n",
    "- No specific range boundary. \n",
    "\n",
    "$$\n",
    "x_{scaled} = \\frac{x - \\hat{x}}{\\sigma_{x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are doing regression and don't have a scaling boundary requirement and there are probably outliers, we'll use standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "example_SS = StandardScaler()\n",
    "example_SS = example_SS.fit_transform(np.array(housing.total_rooms).reshape(-1,1))\n",
    "\n",
    "plt.hist(housing.total_rooms, bins=100)\n",
    "plt.title('Total Rooms', loc='left')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(example_SS, bins=100)\n",
    "plt.title('Total Rooms - Standardized', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Test Splits\n",
    "\n",
    ">Put the test data aside and never look at it again.\n",
    "\n",
    "- All of the feature transformations and model training should be on the training data.  \n",
    "- In production, you wouldn't exactly know what the incoming data would look like ahead of time.  \n",
    "- If you use the test data to inform **any** of the feature transformations or modeling, then you are letting that test data leak into the training data and that will (may) bias your evaluations. This is called **leakage** or **data snooping** - both are not good.\n",
    "\n",
    "### Simpliest form is splitting your data into two parts:\n",
    "- Training: will base feature transforms and modeling on this. \n",
    "- Test: evaluate the models on this data. \n",
    "\n",
    ">There are more robust methods that we'll talk about later. You can think of this simple splitting as a quick and dirty way to evaluate performance, but it isn't a methodology you'd want to use to estimate what your performance is truly likely to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split off the features and the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = housing.median_house_value\n",
    "\n",
    "features = ['housing_median_age', 'total_rooms', 'total_bedrooms', \n",
    "            'population', 'households', 'median_income', 'ocean_proximity', 'longitude', 'latitude'\n",
    "           ]\n",
    "\n",
    "X = housing[features]\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and test sets\n",
    ">80/20 split is pretty standard, but not universal. For very large datasets, I've heard of 99/1 splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_training, X_test, y_training, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "print(f'Training samples: {X_training.shape[0]:,}')\n",
    "print(f'Test samples: {X_test.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Remember, the test data is only for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "- Fill missing values. \n",
    "- Create dummies for categorical.  \n",
    "- Standardize numerical variables.  \n",
    "- Fit the model.  \n",
    "\n",
    "### Pipelines can be made of collections of pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cat_vars = ['ocean_proximity']\n",
    "num_vars = ['housing_median_age', 'total_rooms', 'total_bedrooms', 'population',\n",
    "            'households', 'median_income', 'longitude', 'latitude']\n",
    "\n",
    "num_pipeline = Pipeline([('impute_missing', SimpleImputer(strategy='median')),\n",
    "                           ('standardize_num', StandardScaler())\n",
    "                        ])\n",
    "\n",
    "cat_pipeline = Pipeline([('impute_missing_cats', SimpleImputer(strategy='most_frequent')),\n",
    "                          ('create_dummies_cats', OneHotEncoder(handle_unknown='ignore', drop='first'))])\n",
    "\n",
    "processing_pipeline = ColumnTransformer(transformers=[('proc_numeric', num_pipeline, num_vars),\n",
    "                                                      ('create_dummies', cat_pipeline, cat_vars)])\n",
    "\n",
    "print(processing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "modeling_pipeline = Pipeline([('data_processing', processing_pipeline), ('lm', LinearRegression())])\n",
    "modeling_pipeline.fit(X_training, y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    ">Really evaluatign the entire preprocessing process and the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = modeling_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the mean squared error, root mean squared error, and $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, housing_predictions)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, housing_predictions)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the test and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(y_test, housing_predictions, 'ro')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "- If this was a perfect model, the residuals would be 0 for all actual/predicted values.  \n",
    "- Residuals should look like white noise across all values - seeing some patterns.  \n",
    "- Some information is leaking into the residuals that the model is capturing.  \n",
    "    - Could be a feature we don't have access to.  \n",
    "    - Could be noise in the data.  \n",
    "    - Could be the underlying relationships are linear.  \n",
    "    - Insert any number of additional explanations.  \n",
    "    \n",
    "There may be some overfitting - the training data fits better than the test data. We can explore other models to see if they are able to reduce the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-variance Tradeoff\n",
    "\n",
    "<img src='files/diagrams/bullseye.png' style=\"width: 600px;\">\n",
    "\n",
    "[Image source](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "\n",
    ">At its root, dealing with bias and variance is really about dealing with **over- and under-fitting**. Bias is reduced and variance is increased in relation to model complexity. As more and more parameters are added to a model, the complexity of the model rises and variance becomes our primary concern while bias steadily falls\n",
    "<br><br>Understanding the Bias-Variance Tradeoff, Fortmann-Roe\n",
    "\n",
    "\n",
    "From Raschka (paraphrased):\n",
    ">Variance measures the consistency (or variability) of the model prediction. If we retrain the model on different subsets of the training get and observe difference results, we say it is subject to high variance.\n",
    "\n",
    ">Bias measures how far off the predictions are from the correct values. This will be error that isn't due to differences in the training datasets.\n",
    "\n",
    "[Bias and variance from Raschka's Evaluation Lecture Notes](https://sebastianraschka.com/pdf/lecture-notes/stat479fs18/08_eval-intro_notes.pdf)\n",
    "\n",
    "\n",
    "<img src='files/diagrams/high-bias.png' style=\"width: 600px;\">\n",
    "\n",
    "\n",
    "<img src='files/diagrams/high-variance.png' style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Usually Triumphs Over the Complex\n",
    "It's a balancing act though. You'll need a minimum level of complexity to capture the relationships in the data.\n",
    "\n",
    "<img src='files/diagrams/biasvariance.png'>\n",
    "\n",
    "[Image source](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "\n",
    "## Potential Options\n",
    "- Include less features.  \n",
    "- Shrinkage methods.  \n",
    "- Data over models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Variance, Forward, and Backward Selection\n",
    "\n",
    "## [Low Variance](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    ">You can automatically omit features with zero-variance (i.e., constants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Forward or Backward Selection](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector)\n",
    "\n",
    "From scikit-learn:\n",
    ">Forward-SFS is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. Concretely, we initially start with zero feature and find the one feature that maximizes a cross-validated score when an estimator is trained on this single feature. Once that first feature is selected, we repeat the procedure by adding a new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the n_features_to_select parameter.\n",
    "\n",
    ">Backward-SFS follows the same idea but works in the opposite direction: instead of starting with no feature and greedily adding features, we start with all the features and greedily remove features from the set. The direction parameter controls whether forward or backward SFS is used.\n",
    "<br><br>In general, forward and backward selection do not yield equivalent results. Also, one may be much faster than the other depending on the requested number of selected features: if we have 10 features and ask for 7 selected features, forward selection would need to perform 7 iterations while backward selection would only need to perform 3.\n",
    "\n",
    ">SFS differs from RFE and SelectFromModel in that it does not require the underlying model to expose a coef_ or feature_importances_ attribute. It may however be slower considering that more models need to be evaluated, compared to the other approaches. For example in backward selection, the iteration going from m features to m - 1 features using k-fold cross-validation requires fitting m * k models, while RFE would require only a single fit, and SelectFromModel always just does a single fit and requires no iterations.\n",
    "\n",
    "[See a *Comparative Study of Techniques for Large-Scale Feature Selection* for more discussion.](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.4369&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Idea\n",
    "- Start with all the features. \n",
    "- Determine the feature provided the least added benefit.    \n",
    "- Remove the above feature.  \n",
    "- Continue until reach the desired number of features or hit a threshold.  \n",
    "\n",
    "### Rational\n",
    "> Automatically select the most relevant subset of features.  \n",
    "\n",
    "> Really only necessary if your models don't support regularization.\n",
    "\n",
    "> Can help with the *Curse of Dimensionality* since it'll generally select a more parsimonious model with dense features.\n",
    "\n",
    "From Machine Learning with Python, SBS (backward) showed a model with 3 features would have achieved 100% accuracy on the validation data set. See pages 137-139.\n",
    "<img src='files/diagrams/04_08.png' style='width: 300px;'>\n",
    "\n",
    "[Image source - Raschka's GitHub; Python for Machine Learning 3rd Edition, Figure 4.8](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch04/images/04_08.png)\n",
    "\n",
    "\n",
    "- I've never used these in practice - there are other ways to guard against overfitting and selecting a more parsimonious model.  \n",
    "- Can add a lot of computational overhead.  \n",
    "\n",
    "[See an example from scikit-learn for code example.](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-diabetes-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    ">Helps solve overfitting (high variance) - having too many parameters (i.e., too complex).  \n",
    "\n",
    ">Also helps with multicolinearity (which we saw) and filtering out noise.  \n",
    "\n",
    "### Types of regularization:\n",
    "- $L1$ (lasso - least absolute shrinkage and selection operator): penalizing the sum of $\\lvert \\beta_{j} \\rvert$  \n",
    "- $L2$ (ridge): penalizing the sum of the $\\beta_{j}$'s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shrinkage with Ridge Regression ($l_2$ Regularization)\n",
    "## We can modify our loss function to penalize complexity\n",
    "Shrinking parameter values to penalize for increased complexity (i.e., parameters with meaningful weights).\n",
    ">Shrinks, but does not eliminate. Coefficients will be non-zero. Will that help when there any many many features?\n",
    "\n",
    "## Reduces the influence of parameters that carry less weight\n",
    "\n",
    "Recall $\\hat\\beta = (X^TX)^{-1}X^{T}y$  \n",
    "Goal: $\\frac{1}{N}\\sum{\\epsilon_{i}^2}$\n",
    "\n",
    "#### We can force parameters to shrink towards zero, effectively reducing their influence by adding a penality to the loss function:\n",
    "$argmin\\frac{1}{N}\\sum{\\epsilon_{i}^2} + \\sum{\\lambda\\beta_{j}^2}$\n",
    "- What happens with $\\lambda=0$?\n",
    "\n",
    "New closed form: $\\hat\\beta = (X^TX+\\lambda{I})^{-1}X^{T}y$ \n",
    "\n",
    "> Ridge has a unique solution, similar to OLS. You can modify the closed form implementation from last class as an independent proof. scikit-learn's implementation uses various optimization methods to solve the loss optimization problem, so it won't be 100% comparable to OLS with $\\alpha=0$, it'll be close though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduces $\\lambda$ - our first (official) hyperparameter!\n",
    "$\\lambda$ controls the amount of the penality, it is bounded between 0 and $\\infty$.\n",
    ">When $\\lambda=0$, ridge will provide the same coefficients as OLS, since the $\\lambda{I}$ will become zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "modeling_pipeline_ridge = Pipeline([('data_processing', processing_pipeline), ('ridge', Ridge(alpha=0))])\n",
    "modeling_pipeline_ridge.fit(X_training, y_training)\n",
    "modeling_pipeline_ridge['ridge'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compared to the below from the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_pipeline['lm'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now to evaluate different $\\lambda$ values\n",
    ">We'll need to evaluate a gradient of $\\lambda$ values to determine the best one to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "alphas = [0, 1, 2, 5, 10, 50]\n",
    "\n",
    "ridge_results = defaultdict(dict)\n",
    "\n",
    "for alph in alphas:\n",
    "    modeling_pipeline_ridge = Pipeline([('data_processing', processing_pipeline), ('ridge', Ridge(alpha=alph))])\n",
    "    modeling_pipeline_ridge.fit(X_training, y_training)\n",
    "    ridge_results['coefficients'][alph] = modeling_pipeline_ridge['ridge'].coef_\n",
    "    ridge_results['training score'][alph] = modeling_pipeline_ridge.score(X_training, y_training)\n",
    "    ridge_results['test score'][alph] = modeling_pipeline_ridge.score(X_test, y_test)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_ridge = pd.DataFrame.from_dict(ridge_results['coefficients'])\n",
    "coefficients_ridge = coefficients_ridge.reset_index()\n",
    "coefficients_ridge = coefficients_ridge.rename(columns={'index':'coefficient_nbr'})\n",
    "coefficients_ridge = coefficients_ridge.melt(id_vars='coefficient_nbr', var_name='alpha', value_name='coefficient')\n",
    "\n",
    "(\n",
    "coefficients_ridge.pivot_table(index='alpha', columns='coefficient_nbr', values='coefficient')\n",
    "    .plot(figsize=(8,4),legend=False)\n",
    ")\n",
    "\n",
    "plt.title('Ridge Coefficients', loc='left')\n",
    "plt.xlabel('Alpha (Regularization Amount)')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes in $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_training_r2 = pd.Series(ridge_results['training score'])\n",
    "ridge_test_r2 = pd.Series(ridge_results['test score'])\n",
    "\n",
    "ridge_training_r2.plot()\n",
    "ridge_test_r2.plot()\n",
    "plt.title('$R^2$ for Ridge Regression')\n",
    "plt.legend(['Training','Test'])\n",
    "plt.xlabel('Alpha (Regularization Level)')\n",
    "plt.ylabel('Percent of Variance Explained')\n",
    "plt.ylim(0.4, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Option is Lasso - Requires Gradient Descent (GD)\n",
    "OLS and Ridge regression have unique solutions (even though scikit-learn uses optimization). In order to talk about some of the other variants, we need to talk about an optimization technique called gradient descent.\n",
    "\n",
    "Gradient descent is an optimization technique that allows us to \"learn\" what the coefficients should be by iteration and continuous improvment. Traditional statisticans don't love this technique since it's not too far away from guessing a bunch of times until you can't guess much better.  \n",
    "\n",
    "<img src='files/diagrams/gs.png' style=\"width: 300px;\">\n",
    "\n",
    "[Image source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.datasciencecentral.com%2Fprofiles%2Fblogs%2Falternatives-to-the-gradient-descent-algorithm&psig=AOvVaw1ki8gWYTrWRy-NKpu7RFgo&ust=1631036623607000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCJiSq6nz6vICFQAAAAAdAAAAABAD)\n",
    "\n",
    "Gradient descent essentially is looking for the local minimums of loss functions that are differentiable.  \n",
    "\n",
    "While least-squares does not a closed-form solution, you can also approximate it using gradient descent. Gradient descent will reappear with other algorithms.\n",
    "\n",
    "GD requires a loss function, which for OLS regression is the sum of squared errors:\n",
    "$$J(w)=\\frac{1}{2}\\sum(y^{(i)} - \\hat{y}^{(i)})^2$$\n",
    "\n",
    "This also be used in logistic regression and neural networks.\n",
    "\n",
    "## Updating weights\n",
    "All of the weights are set simultaneously.\n",
    "\n",
    "1. Initialize weights to 0 or small random numbers. \n",
    "2. For each training example, $x^{i}$:  \n",
    "    a. Compute the output value, $\\hat{y}$.  \n",
    "    b. Update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, m, numIterations):\n",
    "    thetaHistory = list()\n",
    "    \n",
    "    xTrans = x.transpose()\n",
    "    costList = list()\n",
    "    \n",
    "    for i in range(0, numIterations):\n",
    "        # data x feature weights = y_hat\n",
    "        hypothesis = np.dot(x, theta)\n",
    "        # how far we are off\n",
    "        loss = hypothesis - y \n",
    "        # mse\n",
    "        cost = np.sum(loss ** 2) / (2 * m)\n",
    "        costList.append(cost)\n",
    "\n",
    "        # avg gradient per example\n",
    "        gradient = np.dot(xTrans, loss) / m \n",
    "\n",
    "        # update\n",
    "        theta = theta - alpha * gradient\n",
    "        thetaHistory.append(theta)\n",
    "        \n",
    "    return thetaHistory, costList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline = Pipeline([('data_processing', processing_pipeline)])\n",
    "data_pipeline.fit(X_training)\n",
    "\n",
    "gs_training_data = data_pipeline.fit_transform(X_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "start_ts = datetime.datetime.now()\n",
    "betaHistory, costList = gradientDescent(gs_training_data,y_training,\n",
    "                                        theta=np.zeros(gs_training_data.shape[1]),\n",
    "                                        alpha=0.01,\n",
    "                                        m=gs_training_data.shape[0], numIterations=5000)\n",
    "                                                                 \n",
    "end_ts = datetime.datetime.now()\n",
    "\n",
    "print(f'Completed in {end_ts-start_ts}')\n",
    "\n",
    "plt.plot(costList)\n",
    "plt.title(f'Final cost: {costList[-1]:,.2f}', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show changes in $\\beta$ throughout the iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "thetas = defaultdict(list)\n",
    "\n",
    "for i in range(len(betaHistory)):\n",
    "    for j in range(len(betaHistory[i])):\n",
    "        thetas[j].append(betaHistory[i][j])\n",
    "        \n",
    "thetasD = pd.DataFrame.from_dict(thetas)\n",
    "thetasD.plot(legend=False)\n",
    "plt.title('Beta Estimates')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_betas = betaHistory[4999]\n",
    "gs_predictions = np.dot(gs_training_data, gs_betas)\n",
    "\n",
    "plt.plot(y_training, gs_predictions, 'bo', alpha=0.4)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Gradient Descent Regression Fit on Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where things can go wrong:\n",
    "- The learning rate (alpha generally) is really important. If you pick a rate that is too large, you may hop over the minimum and the models will either be very poor or never converge.  \n",
    "- Other thresholds may take some trial and error.  \n",
    "- You can get stuck at a local minima and never find the local maxima.  \n",
    "- You need to know when to stop. It'll keep adjusting coefficients until it reaches an iteration limit or a derivative threshold. Too long and overfitting could occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression ($l_1$ Regularization)\n",
    "No closed form solution - will need to use optimization techniques regardless.  \n",
    "\n",
    "$$\n",
    "J(w)_{lasso}=\\sum{(y^{(i)}-\\hat{y}^{(i)})^2+\\lambda \\lvert \\lvert w \\rvert \\rvert_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L1:\\lambda \\lvert \\lvert w \\rvert \\rvert_1 = \\lambda \\sum{\\lvert w_j \\rvert}\n",
    "$$\n",
    "\n",
    "Could modify the gradient descent with the above, but we'll let sci-kit learn handle it.\n",
    "\n",
    "## Differences from Ridge\n",
    "\n",
    "<img src='files/diagrams/ridge-v-lasso.png'>\n",
    "\n",
    "[Introduction to Statistical Learning, Figure 3.11](https://www.statlearning.com)\n",
    ">Estimation picture for the lasso (left) and ridge regression (right). Shown are contours of the error and constraint functions. The solid blue areas are the constraint regions $\\lvert \\beta_1 \\rvert \\leq t$ and $\\beta_{1}^2 + \\beta_{2}^2 \\leq t^2$, respectivity, while the red ellipses are the contours of the least squares error function.\n",
    "\n",
    "#### Explanation from Raschka (Python Machine Learning 3rd Edition, Chapter 4, pages 129-131):\n",
    "> We can think of regularization as adding a penalty term to the cost function to encourage smaller weighs; in other words, we penalize large weights. Thus, by increasing the regularization strength via the regularization parameter, we shrink the weights toward zero and decrease the dependence of our model on the training data.\n",
    "\n",
    "> The shaded regions of represent the regularization \"budget\" - the combination of the weights cannot exceed those limits. As the regularization term increases, so does the area of that shaded region.\n",
    "\n",
    "[See Elements of Statistical Learning Section 3.4 for a more thorough discussion.](https://web.stanford.edu/~hastie/ElemStatLearn/)\n",
    "\n",
    "### TL:DR - Lasso can create sparse models, Ridge cannot.\n",
    "Ridge will have non-zero estimates for its $\\beta$ values, and lasso can result in some $\\beta$ values equal to zero (i.e., sparse).\n",
    "\n",
    "- Lasso should provide better protection to overfitting than Ridge and OLS.  \n",
    "- Can also be a technique by itself for feature selection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from collections import defaultdict\n",
    "\n",
    "alphas = [1, 2, 5, 10, 50]\n",
    "\n",
    "lasso_results = defaultdict(dict)\n",
    "\n",
    "for alph in alphas:\n",
    "    modeling_pipeline_lasso = Pipeline([('data_processing', processing_pipeline), ('lasso', Lasso(alpha=alph))])\n",
    "    modeling_pipeline_lasso.fit(X_training, y_training)\n",
    "    lasso_results['coefficients'][alph] = modeling_pipeline_lasso['lasso'].coef_\n",
    "    lasso_results['training score'][alph] = modeling_pipeline_lasso.score(X_training, y_training)\n",
    "    lasso_results['test score'][alph] = modeling_pipeline_lasso.score(X_test, y_test)\n",
    "    \n",
    "coefficients_lasso = pd.DataFrame.from_dict(lasso_results['coefficients'])\n",
    "coefficients_lasso = coefficients_lasso.reset_index()\n",
    "coefficients_lasso = coefficients_lasso.rename(columns={'index':'coefficient_nbr'})\n",
    "coefficients_lasso = coefficients_lasso.melt(id_vars='coefficient_nbr', var_name='alpha', value_name='coefficient')\n",
    "\n",
    "coefficients_lasso.pivot_table(index='alpha', columns='coefficient_nbr', values='coefficient').plot(figsize=(8,4))\n",
    "plt.title('Lasso Coefficients', loc='left')\n",
    "plt.xlabel('Alpha (Regularization Amount)')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_ridge.query('coefficient_nbr == 9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_lasso.query('coefficient_nbr == 9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lasso shrunk to $0$ and ridge did not!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading\n",
    "\n",
    "[Spurious correlations (High $r^2$ useless relationships)](https://www.tylervigen.com/spurious-correlations)\n",
    "<br>[Mostly Harmless Econometrics](https://www.mostlyharmlesseconometrics.com)\n",
    "<br>[Additional examples from statsmodels](https://www.statsmodels.org/stable/examples/notebooks/generated/ols.html)\n",
    "<br> [Common pitfalls](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html)\n",
    "<br>[Andrew Ng on Regularization](https://www.youtube.com/watch?v=NyG-7nRpsW8&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=6)\n",
    "<br>[Great notes from Goker Erdogan](https://github.com/gokererdogan/JaverianaMLCourse/blob/master/Lectures/05.pdf).\n",
    "<br>[Raschka on Regularization Performance](https://sebastianraschka.com/faq/docs/regularized-logistic-regression-performance.html)\n",
    "<br>[Raschka's Linear Regression Notes](http://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/)\n",
    "\n",
    "From Raschka's post:\n",
    ">Regularization does NOT improve the performance on the data set that the algorithm used to learn the model parameters (feature weights). However, it can improve the generalization performance, i.e., the performance on new, unseen data, which is exactly what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
